{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi0101/ML-TPs/blob/main/RL_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkd375upWYok"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
        "# \n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of 6.S191 must\n",
        "# reference:\n",
        "#\n",
        "# © MIT 6.S191: Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoXYKhfZMHiw"
      },
      "source": [
        "#Reinforcement Learning\n",
        "\n",
        "L'apprentissage par renforcement (RL) est un sous-ensemble de l'apprentissage automatique qui pose les problèmes d'apprentissage comme des interactions entre les agents et les environnements. Il suppose souvent que les agents n'ont aucune connaissance préalable du monde et qu'ils doivent apprendre à naviguer dans les environnements en optimisant une fonction de récompense. Dans un environnement, un agent peut entreprendre certaines actions et recevoir un retour, sous la forme de récompenses positives ou négatives, par rapport à sa décision. En tant que telle, la boucle de rétroaction d'un agent s'apparente quelque peu à l'idée d'\"essais et erreurs\", ou à la manière dont un enfant peut apprendre à distinguer les \"bonnes\" et les \"mauvaises\" actions.\n",
        "\n",
        "En termes pratiques, notre agent RL interagira avec l'environnement en effectuant une action à chaque pas de temps, en recevant une récompense correspondante et en mettant à jour son état en fonction de ce qu'il a \"appris\".  \n",
        "\n",
        "![texte alt](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "Bien que l'objectif ultime de l'apprentissage par renforcement soit d'apprendre aux agents à agir dans le monde réel et physique, les environnements simulés - comme les jeux et les moteurs de simulation - constituent un terrain d'essai pratique pour le développement d'algorithmes et d'agents d'apprentissage par renforcement.\n",
        "\n",
        "Dans les laboratoires précédents, nous avons exploré les tâches d'apprentissage supervisé (avec LSTMs, CNNs) et non supervisé / semi-supervisé (avec VAEs). L'apprentissage par renforcement est fondamentalement différent, dans la mesure où nous formons un algorithme d'apprentissage profond pour régir les actions de notre agent RL, qui essaie, dans son environnement, de trouver la manière optimale d'atteindre un objectif. L'objectif de la formation d'un agent RL est de déterminer la meilleure étape à suivre pour obtenir le meilleur gain ou rendement final. Dans ce laboratoire, nous nous concentrons sur la construction d'un algorithme d'apprentissage par renforcement pour maîtriser deux environnements différents avec une complexité variable. \n",
        "\n",
        "1.   **Cartpole** :   Équilibrer un poteau, dépassant d'un chariot, dans une position verticale en déplaçant seulement la base à gauche ou à droite. Environnement avec un espace d'observation de faible dimension.\n",
        "\n",
        "\n",
        "\n",
        "C'est parti ! Tout d'abord, nous allons importer TensorFlow, le paquetage du cours et certaines dépendances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR9QHuleJ9Bh"
      },
      "outputs": [],
      "source": [
        "# Import Tensorflow 2.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Download and import the MIT 6.S191 package\n",
        "!printf \"Installing MIT deep learning package... \"\n",
        "!pip install --upgrade git+https://github.com/aamini/introtodeeplearning.git &> /dev/null\n",
        "!echo \"Done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvdePP-VyVWp"
      },
      "outputs": [],
      "source": [
        "#Install some dependencies for visualizing the agents\n",
        "!apt-get install -y xvfb python-opengl x11-utils &> /dev/null\n",
        "!pip install gym pyvirtualdisplay scikit-video ffio pyrender &> /dev/null\n",
        "!pip install tensorflow_probability==0.12.0 &> /dev/null\n",
        "import os\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import base64, io, os, time, gym\n",
        "import IPython, functools\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import mitdeeplearning as mdl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmrHSiXKTXTY"
      },
      "source": [
        "Avant de nous y plonger, prenons un peu de recul et décrivons notre approche, qui est généralement applicable aux problèmes d'apprentissage par renforcement en général :\n",
        "\n",
        "1. **Initialiser notre environnement et notre agent** : ici nous allons décrire les différentes observations et actions que l'agent peut faire dans l'environnement.\n",
        "2. **Définir la mémoire de notre agent** : cela va permettre à l'agent de se souvenir de ses actions, observations et récompenses passées.\n",
        "3. **Définir une fonction de récompense** : décrit la récompense associée à une action ou une séquence d'actions.\n",
        "4. **Définir l'algorithme d'apprentissage** : il sera utilisé pour renforcer les bons comportements de l'agent et décourager les mauvais comportements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT7YL8KBJIIc"
      },
      "source": [
        "# Cartpole\n",
        "\n",
        "## 1 Définir l'environnement et l'agent de Cartpole\n",
        "\n",
        "### Environnement \n",
        "\n",
        "Afin de modéliser l'environnement de la tâche Cartpole, nous utiliserons une boîte à outils développée par OpenAI appelée [OpenAI Gym] (https://gym.openai.com/). Elle fournit plusieurs environnements prédéfinis pour l'entraînement et le test des agents d'apprentissage par renforcement, y compris ceux des tâches classiques de contrôle physique, des jeux vidéo Atari et des simulations robotiques. Pour accéder à l'environnement Cartpole, nous pouvons utiliser `env = gym.make(\"CartPole-v0\")`, auquel nous avons eu accès lorsque nous avons importé le paquet `gym`. Nous pouvons instancier différents [environnements] (https://gym.openai.com/envs/#classic_control) en passant le nom de l'environnement à la fonction `make`.\n",
        "\n",
        "Un problème que nous pouvons rencontrer lors du développement d'algorithmes de RL est que de nombreux aspects du processus d'apprentissage sont intrinsèquement aléatoires : l'initialisation des états du jeu, les changements dans l'environnement et les actions de l'agent. En tant que tel, il peut être utile de définir une \"graine\" initiale pour l'environnement afin d'assurer un certain niveau de reproductibilité. Tout comme vous pourriez utiliser `numpy.random.seed`, nous pouvons appeler la fonction comparable dans gym, `seed`, avec notre environnement défini pour s'assurer que les variables aléatoires de l'environnement sont initialisées de la même manière à chaque fois.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quv9SC0iIYFm"
      },
      "outputs": [],
      "source": [
        "### Instantiate the Cartpole environment ###\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhEITUcKK455"
      },
      "source": [
        "Dans Cartpole, un poteau est attaché par une articulation non actionnée à un chariot, qui se déplace le long d'une piste sans frottement. Le poteau commence debout et le but est de l'empêcher de tomber. Le système est contrôlé en appliquant une force de +1 ou -1 au chariot. Une récompense de +1 est fournie pour chaque pas de temps pendant lequel le poteau reste debout. L'épisode se termine lorsque le poteau est à plus de 15 degrés de la verticale ou que le chariot se déplace à plus de 2,4 unités du centre de la piste. Un résumé visuel de l'environnement cartpole est illustré ci-dessous :\n",
        "\n",
        "<img width=\"400px\" src=\"https://danielpiedrahita.files.wordpress.com/2017/02/cart-pole.png\"></img>\n",
        "\n",
        "Compte tenu de cette configuration de l'environnement et de l'objectif du jeu, nous pouvons réfléchir à : 1) quelles observations aident à définir l'état de l'environnement ; 2) quelles actions l'agent peut entreprendre.\n",
        "\n",
        "Considérons d'abord l'espace d'observation. Dans cet environnement Cartpole, nos observations sont :\n",
        "\n",
        "1. Emplacement du chariot\n",
        "2. Vitesse du chariot\n",
        "3. Angle de poteau\n",
        "4. Taux de rotation des pôles\n",
        "\n",
        "On peut confirmer la taille de l'espace en interrogeant l'espace d'observation de l'environnement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVJaEcbdIX82"
      },
      "outputs": [],
      "source": [
        "n_observations = env.observation_space\n",
        "print(\"Environment has observation space =\", n_observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZibGgjrALgPM"
      },
      "source": [
        "Deuxièmement, nous considérons l'espace d'action. A chaque pas de temps, l'agent peut se déplacer soit à droite soit à gauche. Là encore, nous pouvons confirmer la taille de l'espace d'action en interrogeant l'environnement :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc9SIPxBIXrm"
      },
      "outputs": [],
      "source": [
        "n_actions = env.action_space.n\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPfHME8aRKkb"
      },
      "source": [
        "### 1.Agent Cartpole\n",
        "\n",
        "Maintenant que nous avons instancié l'environnement et compris la dimensionnalité des espaces d'observation et d'action, nous sommes prêts à définir notre agent. Dans l'apprentissage par renforcement profond, un réseau neuronal profond définit l'agent. Ce réseau prendra en entrée une observation de l'environnement et produira en sortie la probabilité de prendre chacune des actions possibles. Puisque Cartpole est défini par un espace d'observation de faible dimension, un simple réseau de neurones de type feed-forward devrait bien fonctionner pour notre agent. Nous allons le définir en utilisant l'API `Sequential`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-o_XK4oQ4eu"
      },
      "outputs": [],
      "source": [
        "### Define the Cartpole agent ###\n",
        "\n",
        "# Defines a feed-forward neural network\n",
        "def create_cartpole_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # First Dense layer\n",
        "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "        \n",
        "        # TODO: Define the last Dense layer, which will provide the network's output.\n",
        "        # Think about the space the agent needs to act in!\n",
        "        tf.keras.layers.Dense(units=n_actions, activation=None) # TODO\n",
        "        # ['''TODO''' Dense layer to output action probabilities]\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "cartpole_model = create_cartpole_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5D5NSIYS2IW"
      },
      "source": [
        "Maintenant que nous avons défini l'architecture de base du réseau, nous allons définir une *fonction d'action* qui exécute un passage vers l'avant à travers le réseau, étant donné un ensemble d'observations, et échantillonne la sortie. Cet échantillonnage des probabilités de sortie sera utilisé pour sélectionner la prochaine action de l'agent. Nous ajouterons également un support pour que la fonction `choose_action` puisse traiter soit une seule observation, soit un lot d'observations.\n",
        "\n",
        "**Nous utiliserons cette fonction pour l'apprentissage d'algorithmes de contrôle pour Cartpole, mais elle est également applicable à d'autres tâches de RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_vVZRr8Q4R_"
      },
      "outputs": [],
      "source": [
        "### Define the agent's action function ###\n",
        "\n",
        "# Function that takes observations as input, executes a forward pass through model, \n",
        "#   and outputs a sampled action.\n",
        "# Arguments:\n",
        "#   model: the network that defines our agent\n",
        "#   observation: observation(s) which is/are fed as input to the model\n",
        "#   single: flag as to whether we are handling a single observation or batch of\n",
        "#     observations, provided as an np.array\n",
        "# Returns:\n",
        "#   action: choice of agent action\n",
        "def choose_action(model, observation, single=True):\n",
        "    # add batch dimension to the observation if only a single example was provided\n",
        "    observation = np.expand_dims(observation, axis=0) if single else observation\n",
        "\n",
        "    '''TODO: feed the observations through the model to predict the log probabilities of each possible action.'''\n",
        "    logits = model.predict(observation) # TODO\n",
        "    # logits = model.predict('''TODO''')\n",
        "\n",
        "    '''TODO: Choose an action from the categorical distribution defined by the log \n",
        "       probabilities of each possible action.'''\n",
        "    action = tf.random.categorical(logits, num_samples=1)\n",
        "    # action = ['''TODO''']\n",
        "\n",
        "    action = action.numpy().flatten()\n",
        "\n",
        "    return action[0] if single else action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tR9uAWcTnkr"
      },
      "source": [
        "## 2 Définir la mémoire de l'agent\n",
        "\n",
        "Maintenant que nous avons instancié l'environnement et défini l'architecture du réseau d'agents et la fonction d'action, nous sommes prêts à passer à l'étape suivante de notre workflow RL :\n",
        "1. **Initialiser notre environnement et notre agent** : nous allons décrire ici les différentes observations et actions que l'agent peut effectuer dans l'environnement.\n",
        "2. **Définir la mémoire de notre agent** : cela permettra à l'agent de se souvenir de ses actions, observations et récompenses passées.\n",
        "3. **Définir l'algorithme d'apprentissage** : il sera utilisé pour renforcer les bons comportements de l'agent et décourager les mauvais.\n",
        "\n",
        "Dans l'apprentissage par renforcement, l'apprentissage a lieu parallèlement à l'action de l'agent dans l'environnement ; un *épisode* désigne une séquence d'actions qui aboutit à un état final, tel que la chute du poteau ou le crash du chariot. L'agent devra se souvenir de toutes ses observations et actions, de sorte qu'à la fin d'un épisode, il puisse apprendre à \"renforcer\" les bonnes actions et à punir les actions indésirables via l'entraînement. Notre première étape est de définir un simple tampon `Memory` qui contient les observations de l'agent, ses actions, et les récompenses reçues pour un épisode donné. Nous ajouterons également le support pour combiner une liste d'objets `Memory` en un seul `Memory`. Ceci sera très utile pour le batching, ce qui vous aidera à accélérer l'entraînement plus tard dans le laboratoire.\n",
        "\n",
        "**Une fois de plus, notez la modularité de cette mémoire tampon - elle peut et sera appliquée à d'autres tâches de RL aussi!**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MM6JwXVQ4JG"
      },
      "outputs": [],
      "source": [
        "### Agent Memory ###\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self): \n",
        "        self.clear()\n",
        "\n",
        "  # Resets/restarts the memory buffer\n",
        "    def clear(self): \n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "  # Add observations, actions, rewards to memory\n",
        "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
        "        self.observations.append(new_observation)\n",
        "        '''TODO: update the list of actions with new action'''\n",
        "        self.actions.append(new_action) # TODO\n",
        "        # ['''TODO''']\n",
        "        '''TODO: update the list of rewards with new reward'''\n",
        "        self.rewards.append(new_reward) # TODO\n",
        "        # ['''TODO''']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.actions)\n",
        "\n",
        "# Instantiate a single Memory buffer\n",
        "memory = Memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YhtPaUVj5m"
      },
      "source": [
        "## 3 Fonction de récompense\n",
        "\n",
        "Nous sommes presque prêts à commencer l'algorithme d'apprentissage de notre agent ! L'étape suivante consiste à calculer les récompenses de notre agent lorsqu'il agit dans l'environnement. Puisque nous (et l'agent) ne savons pas si et quand le jeu ou la tâche se terminera (c'est-à-dire quand le poteau tombera), il est utile de mettre l'accent sur l'obtention de récompenses **maintenant** plutôt que plus tard dans le futur - c'est l'idée d'actualisation. C'est un concept similaire à celui de l'actualisation de l'argent dans le cas des intérêts. Rappelez-vous du cours, nous utilisons l'actualisation des récompenses pour privilégier l'obtention des récompenses maintenant plutôt que plus tard dans le futur. L'idée d'actualiser les récompenses est similaire à l'actualisation de l'argent dans le cas des intérêts.\n",
        "\n",
        "Pour calculer la récompense cumulée attendue, appelée **rendement**, à un moment donné d'un épisode d'apprentissage, nous additionnons les récompenses actualisées attendues à ce moment $t$, au sein d'un épisode d'apprentissage, et en nous projetant dans le futur. Nous définissons le rendement (récompense cumulative) à un pas de temps $t$, $R_{t}$ comme :\n",
        "\n",
        ">$R_{t}=\\sum_{k=0}^\\infty\\gamma^kr_{t+k}$\n",
        "\n",
        "où $0 < \\gamma < 1$ est le facteur d'actualisation et $r_{t}$ est la récompense au pas de temps $t$, et l'indice $k$ incrémente la projection dans le futur au sein d'un seul épisode d'apprentissage. Intuitivement, on peut penser que cette fonction déprécie toute récompense reçue à des étapes temporelles ultérieures, ce qui obligera l'agent à donner la priorité à l'obtention de récompenses maintenant. Puisque nous ne pouvons pas étendre les épisodes à l'infini, en pratique, le calcul sera limité au nombre de pas de temps dans un épisode - après quoi la récompense est supposée être nulle.\n",
        "\n",
        "Prenez note de la forme de cette somme -- nous devrons faire preuve d'ingéniosité dans l'implémentation de cette fonction. Plus précisément, nous devrons initialiser un tableau de zéros, dont la longueur correspond au nombre de pas de temps, et le remplir avec les valeurs réelles de la récompense actualisée au fur et à mesure que nous parcourons les récompenses de l'épisode, qui auront été enregistrées dans la mémoire de l'agent. Ce qui nous intéresse en fin de compte, c'est de savoir quelles actions sont les meilleures par rapport aux autres actions entreprises dans cet épisode. Nous normaliserons donc nos récompenses calculées en utilisant la moyenne et l'écart type des récompenses de l'épisode d'apprentissage.\n",
        "\n",
        "Nous utiliserons cette définition de la fonction de récompense dans les deux parties du laboratoire, alors assurez-vous de l'avoir exécutée !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Q2OFYtQ32X"
      },
      "outputs": [],
      "source": [
        "### Reward function ###\n",
        "\n",
        "# Helper function that normalizes an np.array x\n",
        "def normalize(x):\n",
        "    x -= np.mean(x)\n",
        "    x /= np.std(x)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "# Compute normalized, discounted, cumulative rewards (i.e., return)\n",
        "# Arguments:\n",
        "#   rewards: reward at timesteps in episode\n",
        "#   gamma: discounting factor\n",
        "# Returns:\n",
        "#   normalized discounted reward\n",
        "def discount_rewards(rewards, gamma=0.95): \n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    R = 0\n",
        "    for t in reversed(range(0, len(rewards))):\n",
        "        # update the total discounted reward\n",
        "        R = R * gamma + rewards[t]\n",
        "        discounted_rewards[t] = R\n",
        "      \n",
        "    return normalize(discounted_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzbY-mjGYcmt"
      },
      "source": [
        "## 4.Algorithme d'apprentissage\n",
        "\n",
        "Maintenant nous pouvons commencer à définir l'algorithme d'apprentissage qui sera utilisé pour renforcer les bons comportements de l'agent et décourager les mauvais comportements. Dans ce laboratoire, nous nous concentrerons sur les méthodes de gradient de politique qui visent à **maximiser** la probabilité d'actions qui résultent en de grandes récompenses. De manière équivalente, cela signifie que nous voulons **minimiser** la probabilité négative de ces mêmes actions. Nous y parvenons en **scalant** simplement les probabilités par les récompenses qui leur sont associées, ce qui amplifie effectivement la probabilité des actions qui donnent lieu à des récompenses importantes.\n",
        "\n",
        "Puisque la fonction logarithmique est monotone et croissante, cela signifie que minimiser la **vraisemblance négative** est équivalent à minimiser la **vraisemblance logarithmique négative**.  Rappelons que nous pouvons facilement calculer la log-vraisemblance négative d'une action discrète en évaluant son [entropie croisée softmax] (https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). Comme dans l'apprentissage supervisé, nous pouvons utiliser des méthodes de descente de gradient stochastique pour obtenir la minimisation souhaitée. \n",
        "\n",
        "Commençons par définir la fonction de perte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsgZ3IDCY_Zn"
      },
      "outputs": [],
      "source": [
        "### Loss function ###\n",
        "\n",
        "# Arguments:\n",
        "#   logits: network's predictions for actions to take\n",
        "#   actions: the actions the agent took in an episode\n",
        "#   rewards: the rewards the agent received in an episode\n",
        "# Returns:\n",
        "#   loss\n",
        "def compute_loss(logits, actions, rewards): \n",
        "    '''TODO: complete the function call to compute the negative log probabilities'''\n",
        "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits, labels=actions) # TODO\n",
        "    # neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "    #    logits='''TODO''', labels='''TODO''')\n",
        "  \n",
        "    '''TODO: scale the negative log probability by the rewards'''\n",
        "    loss = tf.reduce_mean( neg_logprob * rewards ) # TODO\n",
        "    # loss = tf.reduce_mean('''TODO''')\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr5vQ9fqbPpp"
      },
      "source": [
        "Utilisons maintenant la fonction de perte pour définir une étape de formation de notre algorithme d'apprentissage. Voici une définition très généralisable que nous allons utiliser "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_50ada7nbZ7L"
      },
      "outputs": [],
      "source": [
        "### Training step (forward and backpropagation) ###\n",
        "\n",
        "def train_step(model, loss_function, optimizer, observations, actions, discounted_rewards, custom_fwd_fn=None):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward propagate through the agent network\n",
        "        if custom_fwd_fn is not None:\n",
        "            prediction = custom_fwd_fn(observations)\n",
        "        else: \n",
        "            prediction = model(observations)\n",
        "\n",
        "        '''TODO: call the compute_loss function to compute the loss'''\n",
        "        loss = loss_function(prediction, actions, discounted_rewards) # TODO\n",
        "        # loss = loss_function('''TODO''', '''TODO''', '''TODO''')\n",
        "\n",
        "    '''TODO: run backpropagation to minimize the loss using the tape.gradient method. \n",
        "             Unlike supervised learning, RL is *extremely* noisy, so you will benefit \n",
        "             from additionally clipping your gradients to avoid falling into \n",
        "             dangerous local minima. After computing your gradients try also clipping\n",
        "             by a global normalizer. Try different clipping values, usually clipping \n",
        "             between 0.5 and 5 provides reasonable results. '''\n",
        "    grads = tape.gradient(loss, model.trainable_variables) # TODO\n",
        "    # grads = tape.gradient('''TODO''', '''TODO''')\n",
        "    grads, _ = tf.clip_by_global_norm(grads, 2)\n",
        "    # grads, _ = tf.clip_by_global_norm(grads, '''TODO''')\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsjKXh6BcgjR"
      },
      "source": [
        "## 5 Exécuter cartpole !\n",
        "\n",
        "N'ayant aucune connaissance préalable de l'environnement, l'agent va commencer à apprendre comment équilibrer la perche sur le chariot en se basant uniquement sur les informations reçues de l'environnement ! Après avoir défini comment notre agent peut se déplacer, comment il reçoit de nouvelles observations et comment il met à jour son état, nous allons voir comment il apprend progressivement une politique d'actions pour optimiser l'équilibre du poteau aussi longtemps que possible. Pour ce faire, nous suivrons l'évolution des récompenses en fonction de l'entraînement - comment les récompenses devraient-elles changer au fur et à mesure de l'entraînement ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hZ7E6JOJ9Bn"
      },
      "outputs": [],
      "source": [
        "## Training parameters ##\n",
        "## Re-run this cell to restart training from scratch ##\n",
        "\n",
        "# TODO: Learning rate and optimizer\n",
        "learning_rate = 1e-3\n",
        "# learning_rate = '''TODO'''\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "# optimizer = '''TODO'''\n",
        "\n",
        "# instantiate cartpole agent\n",
        "cartpole_model = create_cartpole_model()\n",
        "\n",
        "# to track our progress\n",
        "smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.95)\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Rewards')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmOzc2rrcn8Q",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "## Cartpole training! ##\n",
        "## Note: stoping and restarting this cell will pick up training where you\n",
        "#        left off. To restart training you need to rerun the cell above as \n",
        "#        well (to re-initialize the model and optimizer)\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "for i_episode in range(500):\n",
        "\n",
        "    plotter.plot(smoothed_reward.get())\n",
        "    # Restart the environment\n",
        "    observation = env.reset()\n",
        "    memory.clear()\n",
        "\n",
        "    while True:\n",
        "        # using our observation, choose an action and take it in the environment\n",
        "        action = choose_action(cartpole_model, observation)\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        # add to memory\n",
        "        memory.add_to_memory(observation, action, reward)\n",
        "\n",
        "        # is the episode over? did you crash or do so well that you're done?\n",
        "        if done:\n",
        "            # determine total reward and keep a record of this\n",
        "            total_reward = sum(memory.rewards)\n",
        "            smoothed_reward.append(total_reward)\n",
        "          \n",
        "            # initiate training - remember we don't know anything about how the \n",
        "            #   agent is doing until it has crashed!\n",
        "            g = train_step(cartpole_model, compute_loss, optimizer, \n",
        "                       observations=np.vstack(memory.observations),\n",
        "                       actions=np.array(memory.actions),\n",
        "                       discounted_rewards = discount_rewards(memory.rewards))\n",
        "          \n",
        "            # reset the memory\n",
        "            memory.clear()\n",
        "            break\n",
        "        # update our observatons\n",
        "        observation = next_observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkcUtGF1VE-K"
      },
      "source": [
        "Pour avoir une idée de la façon dont notre agent s'est débrouillé, nous pouvons enregistrer une vidéo du modèle entraîné travaillant sur l'équilibre du poteau. Il s'agit d'un tout nouvel environnement que l'agent n'a jamais vu auparavant !\n",
        "\n",
        "Affichons la vidéo enregistrée pour voir comment notre agent s'est comporté !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAYBkv6Zbk0J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "matplotlib.use('Agg') \n",
        "saved_cartpole = mdl.lab3.save_video_of_model(cartpole_model, \"CartPole-v1\")\n",
        "mdl.lab3.play_video(saved_cartpole)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSbVNDpaVb3_"
      },
      "source": [
        "Quelles sont les performances de l'agent ? Pourriez-vous le former pendant une période plus courte tout en obtenant de bons résultats ? Pensez-vous qu'un entrainement plus long serait encore plus efficace ? "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x_wlW0USt8y8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RL_Cartpole.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}